# Cognitive Organism Configuration
# Magic numbers justified by evolutionary goals and architectural constraints.

corpus:
  - enwik8

model:
  L: 32       # Deepened for complex bit-level reasoning
  R: 8       # Scaled parallelism for higher neural density
  D: 256      # Expanded Brain Scale
  C: 4        # Doubled columns for diverse feature extraction
  memory_depth: 8 # Extended hierarchical memory
  neural_cache_enabled: false # Disable reflex pattern cache for reasoning-first training
  use_fused_cognitive_cycle: true # Enabled for optimized orchestration.
  use_forward_stack: true # Enabled for C++ stacked reasoning path.
  lgh_enabled: true # Enable Liquid Geometric Hypernetwork path.
  lgh_replace_forward_stack: true # Prefer LGH kernel over legacy forward_stack_io.
  lgh_curve_length: 96 # Number of Morton-ordered nodes in a reasoning trajectory.
  lgh_curve_wrap: true # Allow curve trajectories to wrap around manifold end.
  lgh_mask_min_keep: 0.10 # Lower bound on active mDNA gate ratio.
  lgh_mask_max_keep: 0.90 # Upper bound on active mDNA gate ratio.
  lgh_morton_depth: 1 # Z-depth for Morton buffer (LxRxdepth).
  lgh_prefetch_distance: 2 # Prefetch distance (segments ahead) for LGH kernel.
  lgh_align_multiple: 64 # Pad manifold rows to hardware-aligned multiples (cacheline friendly).
  lgh_temporal_bins: 16 # Folded temporal bins for 4D spatiotemporal manifold behavior.
  lgh_temporal_fold_alpha: 0.25 # Strength of temporal fold shift in Morton space.
  lgh_wave_radius: 1 # Wave propagation radius in Morton-neighbor rows.
  lgh_wave_decay: 0.65 # Exponential decay factor for wave propagation strength.
  lgh_trace_decay: 0.90 # Synaptic trace decay per kernel step.
  lgh_trace_gain: 0.20 # Synaptic trace gain injected into manifold pulses.
  lgh_low_entropy_fold_threshold: 0.015 # Skip LGH pulse when temporal delta falls below this threshold.
  lgh_focus_strength: 0.35 # Strength of foveated manifold focus (core vs periphery density).
  lgh_focus_sharpness: 2.0 # Sharpness of foveated focus falloff.
  mes_super_kernel: true # Use C++ MES super-step when available.
  audit_period_steps: 25 # Less frequent deterministic audit cadence.
  audit_random_prob: 0.01 # Small stochastic audit probability.
  hpc_temporal_gate_enabled: true # Scale/skip reasoning when input stream is temporally static.
  hpc_temporal_gate_threshold: 0.08 # Higher means stricter requirement for temporal novelty.
  hpc_temporal_gate_min_scale: 0.55 # Minimum cycle scale under very low temporal activity.
  hpc_temporal_gate_skip_enabled: true # Allow full skip only when surprise+temporal both agree.
  hpc_temporal_gate_skip_scale: 0.40 # Temporal scale threshold that marks "very static".
  hpc_temporal_gate_window: 64 # Recent token window used for temporal activity estimation.
  H_cycles: 4     # Increased cycles for deeper reasoning
  L_cycles: 8     # Increased cycles for higher temporal stability
  episodic_capacity: 1000
  episodic_read_top_k: 3
  episodic_max_scan: 512
  episodic_read_every: 2
  episodic_hybrid_mode: true # Hybrid episodic memory is required (legacy modes removed).
  episodic_dense_mode: true # Cold bank uses dense latent compression.
  episodic_hot_capacity: 256 # Empirically better latency/memory balance on this CPU.
  episodic_hot_top_k: 2 # Use hot memory first.
  episodic_cold_top_k: 2 # Better recall/latency tradeoff than top_k=1 in local benchmarks.
  episodic_hybrid_fallback_threshold: 0.35 # Better local tradeoff than 0.20.
  episodic_key_latent_dim: 64
  episodic_value_latent_dim: 64
  episodic_dense_capacity_multiplier: 1.5
  rms_norm_eps: 1e-5
  rope_theta: 10000.0

training:
  mode: 'hybrid'
  lr: 3e-4
  mes_local_l1: 0.01 # Local MES sparsity/regularization term.
  epochs: 50
  batch_size: 64
  seq_len: 512
  # Epoch sizing tuned for faster iteration on 4GB corpus.
  train_samples_per_epoch: 40000
  val_samples_per_epoch: 4000
  max_epochs_per_phase: 1000 # Safety timeout to prevent infinite loops.
  improve_streak_needed: 0 # Required streak of improvements for phase transition.
  improve_min_delta: 1e-4 # Minimum improvement to count towards a streak.
  sparsity_target: 0.9 # Target sparsity for Refinement phase.
  phase3_end_after: 20 # Not used in 2-phase system, but kept for reference.

survival:
  gamma: 0.01 # Decay rate for usage tracking.
  update_every: 50 # Throttle survival usage updates.
  target_sparsity: 0.9
  lambda_cost: 0.5 # Weight for architectural cost.
  lambda_stability: 0.1 # Weight for stability loss.
  lambda_energy: 0.2 # Weight for energy loss.

learning:
  importance_every: 50 # Frequency of block importance measurement.
  importance_sample_ratio: 0.3 # Ratio of blocks sampled for attribution.
  importance_ema_decay: 0.95 # EMA decay for importance thresholding.
  importance_std_factor: 0.25 # Mean + k*std threshold scale.
  gate_update_every: 50 # Frequency of memory gate training.
  knowledge_gamma: 0.001 # Very slow decay for long-term knowledge map.

runtime:
  cpp_omp_threads: 8 # Favor custom C++ kernels on this 8-core CPU.
  torch_num_threads: 2 # Keep PyTorch intra-op low to reduce contention.
  torch_interop_threads: 1 # Avoid extra inter-op scheduling overhead.
  omega_step_update_every: 250 # Update Omega using train-loss EMA during epoch.
  genome_step_update_every: 5000 # Keep genome evolution infrequent (it is heavier than Omega updates).
  train_loss_ema_decay: 0.98 # Smooth proxy signal used for step-level adaptation.
  sparsity_log_every: 50 # Log true gate sparsity cadence (steps).
  lgh_thermal_freq_min_ghz: 3.0 # Penalize genomes that trigger downclock below this target.
  lgh_thermal_ema_decay: 0.95 # EMA smoothing for thermal pressure signal.
  lgh_thermal_penalty_weight: 0.25 # Weight of thermal penalty in evolution fitness.
  lgh_thermal_sample_every_s: 2.0 # Thermal watchdog sampling cadence.
  lgh_simd_cycle_penalty_weight: 0.15 # Silicon-survival penalty weight for expensive AVX-512 paths.
  lgh_simd_starvation_threshold: 1200.0 # Cycles-per-pulse threshold that triggers starvation pressure.
  lgh_int4_uncertainty_threshold: 0.05 # Below this uncertainty, quantize manifold as effective int4.
  lgh_fp32_uncertainty_threshold: 0.18 # Above this uncertainty, use float32 manifold path.
  strict_cpu_only: true # This codebase uses CPU C++ kernels as the production path.
  virtual_lab_enabled: false # Disable heavy benchmark/event logging by default for training runs.
  preflight_enabled: true # Run a startup smoke-check before long training.
  seed: 1337 # Default reproducible seed for trainer and dataloader sampling.


